{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dde591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the existing data\n",
    "df = pd.read_csv('project_ideas.csv')\n",
    "\n",
    "# Identify duplicate rows based on the 'idea' column\n",
    "duplicates = df[df.duplicated(subset=['idea'], keep=False)]\n",
    "\n",
    "# Display duplicate rows\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Duplicate Rows\", dataframe=duplicates)\n",
    "\n",
    "# Drop duplicates, keeping the first occurrence\n",
    "df_cleaned = df.drop_duplicates(subset=['idea'], keep='first').reset_index(drop=True)\n",
    "\n",
    "# # Display cleaned DataFrame sample\n",
    "# tools.display_dataframe_to_user(name=\"Cleaned Data (First 10 Rows)\", dataframe=df_cleaned.head(10))\n",
    "\n",
    "# Save the cleaned file\n",
    "cleaned_path = 'project_ideas_cleaned.csv'\n",
    "df_cleaned.to_csv(cleaned_path, index=False)\n",
    "\n",
    "cleaned_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d91ac00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight (success_rate_pct) -> Average Productivity Score\n",
      "0.45                 -> 78.44%\n",
      "0.50                 -> 79.92%\n",
      "0.55                 -> 81.4%\n",
      "0.60                 -> 82.89%\n",
      "0.65                 -> 84.37%\n",
      "0.70                 -> 85.85%\n",
      "0.75                 -> 87.34%\n",
      "0.80                 -> 88.82%\n",
      "0.85                 -> 90.31%\n",
      "0.90                 -> 91.79%\n",
      "0.95                 -> 93.27%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the metrics dataset\n",
    "df = pd.read_csv('project_metrics_for_tableau.csv')\n",
    "\n",
    "# Ensure numeric columns are floats\n",
    "df['success_rate_pct'] = df['success_rate_pct'].astype(float)\n",
    "df['avg_time_hr'] = df['avg_time_hr'].astype(float)\n",
    "df['manual_dev_hours'] = df['manual_dev_hours'].astype(float)\n",
    "\n",
    "# Iterate over weight combinations (success weight from 0.0 to 1.0)\n",
    "results = []\n",
    "for w in np.linspace(0, 1, 21):  # 0%, 5%, ..., 100%\n",
    "    prod_score = (\n",
    "        w * df['success_rate_pct'] +\n",
    "        (1 - w) * (1 - df['avg_time_hr'] / df['manual_dev_hours']) * 100\n",
    "    )\n",
    "    avg_score = prod_score.mean()\n",
    "    results.append((w, avg_score))\n",
    "\n",
    "# Filter combos yielding average between 77% and 94%\n",
    "valid = [(w, round(avg, 2)) for w, avg in results if 77 <= avg <= 94]\n",
    "\n",
    "# Display results\n",
    "print(\"Weight (success_rate_pct) -> Average Productivity Score\")\n",
    "for w, avg in valid:\n",
    "    print(f\"{w:.2f}                 -> {avg}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad6f14cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated: project_metrics_for_tableau1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gautam\\AppData\\Local\\Temp\\ipykernel_13500\\3628349513.py:86: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('user_id', group_keys=False).apply(mark_repeats).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load cleaned project ideas\n",
    "df = pd.read_csv('project_ideas_cleaned.csv')\n",
    "\n",
    "# 2. Generate random project dates within a 6-month window\n",
    "date_range = pd.date_range(start='2024-10-01', end='2025-04-30')\n",
    "df['date'] = np.random.choice(date_range, size=len(df))\n",
    "\n",
    "# 3. manual_dev_hours already present\n",
    "\n",
    "# 4. Simulate avg_time_hr as 30%–60% of manual_dev_hours\n",
    "df['avg_time_hr'] = df['manual_dev_hours'] * np.random.uniform(0.2, 0.5, size=len(df))\n",
    "\n",
    "# 5. success_flag: fewer interruptions, based on avg_time ratio\n",
    "ratio = df['avg_time_hr'] / df['manual_dev_hours']\n",
    "df['success_flag'] = np.random.poisson(lam=ratio * 3, size=len(df)).clip(0, 3).astype(int)\n",
    "\n",
    "# 6. success_rate_pct drops 5% per interruption\n",
    "df['success_rate_pct'] = np.clip(100 - df['success_flag'] * 5, 0, 100)\n",
    "\n",
    "# 7. Build monthly cohort users\n",
    "monthly_new = {\n",
    "    '2024-10': 200, '2024-11': 500, '2024-12': 1000,\n",
    "    '2025-01': 2000, '2025-02': 3000, '2025-03': 4000,\n",
    "    '2025-04': 5000\n",
    "}\n",
    "cohort_users, user_counter = {}, 0\n",
    "for month, count in monthly_new.items():\n",
    "    month_ts = pd.to_datetime(month)\n",
    "    users = [f\"U{uid:05d}\" for uid in range(user_counter+1, user_counter+1+count)]\n",
    "    cohort_users[month_ts] = users\n",
    "    user_counter += count\n",
    "\n",
    "# 8. Simulate a signup + revisit event stream\n",
    "events = []\n",
    "end_date = pd.Timestamp('2025-04-30')\n",
    "\n",
    "for signup_month, users in cohort_users.items():\n",
    "    # Spread signups across that month\n",
    "    month_days = pd.date_range(signup_month, signup_month + pd.offsets.MonthEnd(0))\n",
    "    for u in users:\n",
    "        signup = np.random.choice(month_days)\n",
    "        events.append({'user_id': u, 'date': signup})\n",
    "\n",
    "        # Force ~40% of users to return once within 5–30 days\n",
    "        if np.random.rand() < 0.4:\n",
    "            forced = signup + pd.Timedelta(days=int(np.random.uniform(5, 30)))\n",
    "            if forced <= end_date:\n",
    "                events.append({'user_id': u, 'date': forced})\n",
    "\n",
    "        # Further visits (mean inter-arrival = 15 days)\n",
    "        next_visit = signup\n",
    "        while True:\n",
    "            interval = np.random.exponential(scale=15)\n",
    "            next_visit = next_visit + pd.Timedelta(days=interval)\n",
    "            if next_visit > end_date:\n",
    "                break\n",
    "            events.append({'user_id': u, 'date': next_visit})\n",
    "\n",
    "# Build and clean the event DataFrame\n",
    "event_df = (\n",
    "    pd.DataFrame(events)\n",
    "      .drop_duplicates(['user_id','date'])\n",
    "      .assign(date=lambda d: pd.to_datetime(d['date']))\n",
    ")\n",
    "\n",
    "# 9. Assign each project in df to a user_id based on matching event dates\n",
    "def sample_user_for_date(proj_date):\n",
    "    pool = event_df[event_df['date'] == proj_date]['user_id'].values\n",
    "    if len(pool) == 0:\n",
    "        # if no exact-match, fall back to random user\n",
    "        return np.random.choice(event_df['user_id'])\n",
    "    return np.random.choice(pool)\n",
    "\n",
    "df['user_id'] = df['date'].apply(sample_user_for_date)\n",
    "\n",
    "# 10. Recompute repeat_flag per user: any visit within 30 days of previous\n",
    "def mark_repeats(g):\n",
    "    g = g.sort_values('date').copy()\n",
    "    g['gap_days'] = g['date'].diff().dt.days.fillna(999)\n",
    "    g['repeat_flag'] = (g['gap_days'] <= 30).astype(int)\n",
    "    return g.drop(columns='gap_days')\n",
    "\n",
    "df = df.groupby('user_id', group_keys=False).apply(mark_repeats).reset_index(drop=True)\n",
    "\n",
    "# 11. Cost calculations\n",
    "df['baseline_cost_usd'] = df['manual_dev_hours'] * 50    # $50/hr dev rate\n",
    "df['esh_cost_usd']      = df['avg_time_hr'] * 1          # $1/hr platform cost\n",
    "\n",
    "# 12. Productivity Score (using your 70/30 weighting)\n",
    "df['productivity_score'] = (\n",
    "    0.7 * df['success_rate_pct'] +\n",
    "    0.3 * (1 - df['avg_time_hr'] / df['manual_dev_hours']) * 100\n",
    ").clip(0, 100)\n",
    "\n",
    "# 13. Cost savings\n",
    "df['cost_savings_usd'] = df['baseline_cost_usd'] - df['esh_cost_usd']\n",
    "\n",
    "# 14. Save to CSV\n",
    "output_path = 'project_metrics_for_tableau1.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"✅ Generated:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537bbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New 30-day retention: 38.49%\n",
      "Saved smoothed data to project_metrics_smoothed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load your metrics DataFrame\n",
    "df = pd.read_csv('project_metrics_for_tableau1.csv')\n",
    "\n",
    "# 1) Reassign `id` in strict chronological order\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "df['id'] = np.arange(1, len(df) + 1)\n",
    "\n",
    "# 2) “Smooth” user assignment:\n",
    "#    – Cap consecutive projects per user at 5 in any rolling 7‑day window\n",
    "#    – If cap reached, force a “cool‑off” of 7–30 days before next assignment\n",
    "\n",
    "def smooth_user_runs(df):\n",
    "    df = df.sort_values(['user_id', 'date']).copy()\n",
    "    next_allowed = {}   # user_id -> earliest next date allowed\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        uid = row['user_id']\n",
    "        dt  = pd.to_datetime(row['date'])\n",
    "\n",
    "        # initialize\n",
    "        if uid not in next_allowed:\n",
    "            next_allowed[uid] = pd.Timestamp.min\n",
    "\n",
    "        # if user is “too soon,” push date forward to cool-off window\n",
    "        if dt < next_allowed[uid]:\n",
    "            cool_days = np.random.randint(7, 31)\n",
    "            dt = next_allowed[uid] + timedelta(days=cool_days)\n",
    "\n",
    "        # update next_allowed based on 7‑day cap\n",
    "        # count how many events in last 7 days for this uid\n",
    "        recent = [r for r in new_rows if r['user_id']==uid and (dt - r['date']).days < 7]\n",
    "        if len(recent) >= 5:\n",
    "            # reached cap → impose immediate cool‑off\n",
    "            cool_days = np.random.randint(7, 31)\n",
    "            dt = recent[-1]['date'] + timedelta(days=cool_days)\n",
    "\n",
    "        # record this adjusted row\n",
    "        new_rows.append({**row, 'date': dt})\n",
    "\n",
    "        # set next_allowed threshold (no restriction until after this)\n",
    "        next_allowed[uid] = dt\n",
    "\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "df = smooth_user_runs(df)\n",
    "\n",
    "# 3) Recompute `repeat_flag` so only returns ≤30 days count as repeats,\n",
    "#    and older returns are treated as new adoption (flag=0):\n",
    "def recompute_repeat_flag(df):\n",
    "    df = df.sort_values(['user_id', 'date']).copy()\n",
    "    df['repeat_flag'] = 0\n",
    "    last_date = {}\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        uid = row['user_id']\n",
    "        dt  = row['date']\n",
    "\n",
    "        if uid in last_date:\n",
    "            gap = (dt - last_date[uid]).days\n",
    "            df.at[idx, 'repeat_flag'] = 1 if gap <= 30 else 0\n",
    "\n",
    "        last_date[uid] = dt\n",
    "\n",
    "    return df\n",
    "\n",
    "df = recompute_repeat_flag(df)\n",
    "\n",
    "# Save the cleaned & smoothed DataFrame\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "df['id'] = df.index + 1\n",
    "\n",
    "retention = df.groupby('user_id')['repeat_flag'].max().mean() * 100\n",
    "print(f\"New 30-day retention: {retention:.2f}%\")\n",
    "\n",
    "df.to_csv('project_metrics_smoothed.csv', index=False)\n",
    "print(\"Saved smoothed data to project_metrics_smoothed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d12e64c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.97507817234957\n",
      "290.55637150244337\n"
     ]
    }
   ],
   "source": [
    "print(df['productivity_score'].mean())\n",
    "print(df['cost_savings_usd'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b580f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved adjusted CSV with updated dates & repeat_flag after May 1, 2025.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load the smoothed dataset\n",
    "df = pd.read_csv('project_metrics_smoothed.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Split into before/after May 1, 2025\n",
    "cutoff = pd.Timestamp('2025-05-01')\n",
    "pre  = df[df['date'] < cutoff].copy()\n",
    "post = df[df['date'] >= cutoff].copy()\n",
    "\n",
    "# 1) Reassign post‑cutoff dates randomly between May 1–May 20, 2025\n",
    "post_dates = pd.date_range('2025-05-01', '2025-05-20')\n",
    "post['date'] = np.random.choice(post_dates, size=len(post), replace=True)\n",
    "\n",
    "# 2) Recompute repeat_flag on post only (30‑day window against BOTH pre and post)\n",
    "#    First, combine date streams so we can look back into pre for each user\n",
    "combined = pd.concat([pre, post], ignore_index=True)\n",
    "combined = combined.sort_values(['user_id','date']).reset_index(drop=True)\n",
    "\n",
    "# 3) Recalculate repeat_flag for every row, but only overwrite post rows\n",
    "last_seen = {}\n",
    "flags = []\n",
    "for idx, row in combined.iterrows():\n",
    "    uid, dt = row['user_id'], row['date']\n",
    "    if uid in last_seen and (dt - last_seen[uid]).days <= 30:\n",
    "        flags.append(1)\n",
    "    else:\n",
    "        flags.append(0)\n",
    "    last_seen[uid] = dt\n",
    "combined['repeat_flag'] = flags\n",
    "\n",
    "# 4) Split back out and save (pre flags remain unchanged; post flags updated)\n",
    "pre_final  = combined[combined['date'] < cutoff]\n",
    "post_final = combined[combined['date'] >= cutoff]\n",
    "\n",
    "df_final = pd.concat([pre_final, post_final], ignore_index=True)\n",
    "df_final = df_final.sort_values('date').reset_index(drop=True)\n",
    "df_final['id'] = np.arange(1, len(df_final)+1)  # reassign sequential IDs\n",
    "\n",
    "# Write out\n",
    "df_final.to_csv('project_metrics_after_may_adjusted.csv', index=False)\n",
    "print(\"Saved adjusted CSV with updated dates & repeat_flag after May 1, 2025.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9606604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New 30-day retention: 38.49%\n",
      "85.97507817234957\n",
      "290.55637150244337\n"
     ]
    }
   ],
   "source": [
    "retention = df_final.groupby('user_id')['repeat_flag'].max().mean() * 100\n",
    "print(f\"New 30-day retention: {retention:.2f}%\")\n",
    "print(df_final['productivity_score'].mean())\n",
    "print(df_final['cost_savings_usd'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e095d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
